# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q7U5G_d7HzC1NsIos_IE1L-RO9fzYLkV

#Business Problem

A large Toyota car dealership offers purchasers of new Toyota cars the option to buy their used car as part of a trade-in. In particular, a new promotion promises to pay high prices for used Toyota Corolla cars for purchasers of a new car. The dealer then sells the used cars for a small profit. To ensure a reasonable profit, the dealer needs to be able to predict the price that the dealership will get for the used cars.

#Objective
* Exploratory Data Analysis
* Price Prediction

#Data 
* Source - https://www.kaggle.com/datasets/tolgahancepel/toyota-corolla

Target variable :
- **Price:** Offer Price 

Features:

- **Age**: Age in months
- **KM**: Accumulated kilometers on odometer
- **FuelType**: Fuel type (Petrol, Diesel, CNG)
- **HP**: HorsePower
- **MetColor**: Metallic color? (Yes = 1, No = 0)
- **Automatic**: Automatic (Yes = 1, No = 0)
- **CC**: Cylinder volume in cubic centimeters
- **Doors**: Number of doors
- **Weight**: Weight in kilograms
"""

#Library for data analysis
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('ToyotaCorolla.csv.xls')

df.head()

print(df.shape)

df.info()

df.isnull().values.any()

"""* Dataset contains no missing values.
* One categorical column - Fuel Type
"""

df.describe()

pd.DataFrame(((df['FuelType'].value_counts(normalize = True))*100).round(2))

"""88% of the cars are Petrol cars, followed by only 10.8% Diesel cars and 1.18% CNG cars.

Let's visualise the data using graphs.
"""

sns.pairplot(df[['Age','KM','HP','CC','Doors','Weight']]);

plt.figure(figsize=(12,10))
sns.scatterplot(data = df, x = 'Age',y = 'Price', hue = 'FuelType', s = 100)
plt.show()

plt.figure(figsize=(12,10))
sns.scatterplot(data = df, x = 'KM',y = 'Price', hue = 'FuelType', s = 100)
plt.show()

"""Offer price for Diesel cars are higher than Petrol cars, even though they are driven more kilometers."""

plt.figure(figsize=(12,10))
sns.scatterplot(data = df, x = 'Weight',y = 'Price', hue = 'FuelType', s = 100)
plt.show()

plt.figure(figsize=(12,10))
sns.histplot(data = df, x = 'Price')
plt.show()

df.groupby('FuelType')['Price'].agg(['count', 'mean', 'sum']).round(2)

df.groupby('FuelType')['KM'].agg(['mean']).round(2)

df.groupby('FuelType')['HP'].agg(['mean']).round(2)

df.groupby('Automatic')['Price'].agg(['count','mean']).round(2)

df.groupby('Doors')['Price'].agg(['count','mean']).round(2)

"""Takeaways:
- Average offer price for Diesel cars are higher.
- Average KM driven is higher for CNG & Diesel cars.
- CNG cars have highest average HP.
- Automatic cars does not have significant average price difference.
- 3 doors & 5 doors cars are more popular.
"""

plt.figure(figsize=(12,10))
sns.heatmap(df.corr(), annot = True)
plt.show()

"""Highly correlated variables are:
- Age & Price
- Weight & CC

## Price Prediction

Regression models used-
- Linear Regression
- Ridge Regression
- Lasso Regression
- Bayesian Ridge Regression
- Regression tree

Steps:
1. Importing necessary libraries
2. One hot encoding of categorical varaibles using 'get_dummies'
3. Seperating outcome & predictors.
4. Splitting dataset into training & validation.
5. Performing linear regression
6. Deploying forward regression to determine important varaibles.
7. Creating new dataset with only important features.
8. Repeating step 3 & 4, and deploying regression models.
9. Measuring error & accuracy
10. Conclusion
"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import RandomizedSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score, roc_curve, auc
from sklearn.metrics import r2_score
from sklearn.tree import DecisionTreeRegressor

!pip install dmba --quiet
from dmba import regressionSummary, classificationSummary, liftChart, gainsChart, adjusted_r2_score, backward_elimination, AIC_score, BIC_score

from sklearn.linear_model import Ridge, Lasso, LassoCV, BayesianRidge
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor

encoded_df = pd.get_dummies(df, drop_first=True)

encoded_df.head()

outcome = 'Price'
predictors = [s for s in encoded_df if s not in outcome]

X = encoded_df[predictors]
Y = encoded_df[outcome]

train_x, valid_x, train_y, valid_y = train_test_split(X, Y, test_size = .2, random_state = 20)
print(train_x.shape, valid_x.shape, train_y.shape, valid_y.shape)

lm = LinearRegression()
lm.fit(train_x, train_y)

print(lm.intercept_)

lm.score(valid_x, valid_y)

pd.DataFrame(lm.coef_, index = predictors)

regressionSummary(train_y, lm.predict(train_x))

prediction = lm.predict(valid_x)

result = pd.DataFrame({'Predicted': prediction, 'Actual': valid_y, 'Residual': valid_y - prediction})
print(result.head(20))

regressionSummary(valid_y, prediction)

#backward selection 

def train_model(variables):
    if len(variables) == 0:
        return None
    model = LinearRegression()
    model.fit(train_x[variables], train_y)
    return model

def score_model(model, variables):
    if len(variables) == 0:
        return AIC_score(train_y, [train_y.mean()] * len(train_y), model, df=1)
    return AIC_score(train_y, model.predict(train_x[variables]), model)

best_model, best_variables = backward_elimination(train_x.columns, train_model, score_model, verbose=True)

print(best_variables)

best_model.score(valid_x[best_variables], valid_y)

regressionSummary(valid_y, best_model.predict(valid_x[best_variables]))

"""Linear Regression model performs better than backward regression. Linear Regression model has higher r2 and lower RMSE, Hence it has higher accuracy.

Running more regression models with default parameters.
"""

#Ridge Regression 

ridge = Ridge()
ridge.fit(train_x, train_y)

print('Score:', ridge.score(valid_x, valid_y))

regressionSummary(valid_y, ridge.predict(valid_x))

#Lasso Regression 

lasso = Lasso()
lasso.fit(train_x, train_y)

print('Score:', lasso.score(valid_x, valid_y))

regressionSummary(valid_y, lasso.predict(valid_x))

#Lasso_cv Regression 

lasso_cv = LassoCV(cv=5)    #5 is default cv value 
lasso_cv.fit(train_x, train_y)

print('Score:', lasso_cv.score(valid_x, valid_y))

regressionSummary(valid_y, lasso_cv.predict(valid_x))

#Bayesian Ridge Regression 

bayesianRidge = BayesianRidge()
bayesianRidge.fit(train_x, train_y)

print('Score:', bayesianRidge.score(valid_x, valid_y))

regressionSummary(valid_y, bayesianRidge.predict(valid_x))

#Regression Tree

param_grid = {
    'max_depth': list(range(2,20)),
    'min_impurity_decrease': [0.0005, 0.001, 0.005, 0.01],
    'min_samples_split': list(range(5,50))}

grid_search = RandomizedSearchCV(DecisionTreeRegressor(), param_grid, cv=5, n_jobs= -1)

grid_search.fit(train_x, train_y)
print('Best Parameter:' , grid_search.best_params_)

reg_tree = grid_search.best_estimator_ 

print('Score:', reg_tree.score(valid_x, valid_y))

regressionSummary(train_y, reg_tree.predict(train_x))
regressionSummary(valid_y, reg_tree.predict(valid_x))

"""Model performance of of Lasso, Ridge, LassoCV and BayesianRidge regression model is worse than Linear regression. This indicates that this dataset does not benefit from regularization. However, Regression tree is our best performing model with score of .88 and RMSE of 1286.04"""